# notebooks/data_preprocessing.py
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from src.utils import load_config, prepare_data, clean_text
import pickle
import yaml

def main():
    print("ğŸ”§ VERÄ° Ã–N Ä°ÅLEME VE HAZIRLAMA")
    print("=" * 60)
    
    # 1. Config yÃ¼kle
    print("1. YapÄ±landÄ±rma dosyasÄ± yÃ¼kleniyor...")
    config = load_config()
    print(f"   âœ… config.yaml yÃ¼klendi")
    
    # 2. Veriyi yÃ¼kle
    print("\n2. Ham veri yÃ¼kleniyor...")
    data_path = config['paths']['raw_data']
    
    if not os.path.exists(data_path):
        print(f"   âŒ Hata: {data_path} bulunamadÄ±!")
        return
    
    df = pd.read_csv(data_path, encoding='utf-8-sig')
    print(f"   âœ… {len(df)} Ã¶rnek yÃ¼klendi")
    
    # 3. Veriyi incele
    print("\n3. Veri analizi:")
    print(f"   - SÃ¼tunlar: {df.columns.tolist()}")
    print(f"   - Skor aralÄ±ÄŸÄ±: {df['sentiment_score'].min():.1f} - {df['sentiment_score'].max():.1f}")
    print(f"   - Ortalama skor: {df['sentiment_score'].mean():.2f}")
    print(f"   - Kategori daÄŸÄ±lÄ±mÄ±:")
    for cat, count in df['category'].value_counts().items():
        print(f"       {cat}: {count} ({count/len(df)*100:.1f}%)")
    
    # 4. Ã–rnek metinleri gÃ¶ster
    print("\n4. Ã–rnek metinler (temizleme Ã¶ncesi/sonrasÄ±):")
    sample_indices = [0, 100, 500]
    for idx in sample_indices:
        original = df.iloc[idx]['text']
        cleaned = clean_text(original)
        print(f"\n   Ã–rnek {idx+1}:")
        print(f"   Orijinal: {original}")
        print(f"   TemizlenmiÅŸ: {cleaned}")
        print(f"   Skor: {df.iloc[idx]['sentiment_score']} - Kategori: {df.iloc[idx]['category']}")
    
    # 5. GÃ¶rselleÅŸtirmeler
    print("\n5. GÃ¶rselleÅŸtirmeler oluÅŸturuluyor...")
    fig, axes = plt.subplots(2, 3, figsize=(18, 10))
    
    # 5.1 Skor daÄŸÄ±lÄ±mÄ±
    axes[0, 0].hist(df['sentiment_score'], bins=21, edgecolor='black', alpha=0.7, color='skyblue')
    axes[0, 0].axvline(x=0, color='red', linestyle='--', alpha=0.5, label='NÃ¶tr')
    axes[0, 0].set_xlabel('Duygu Skoru')
    axes[0, 0].set_ylabel('Frekans')
    axes[0, 0].set_title('Duygu SkorlarÄ± DaÄŸÄ±lÄ±mÄ±')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    
    # 5.2 Kategori daÄŸÄ±lÄ±mÄ±
    category_counts = df['category'].value_counts().sort_index()
    colors = ['#ff6666', '#ff9999', '#cccccc', '#99ff99', '#00cc00']  # KÄ±rmÄ±zÄ±dan yeÅŸile
    axes[0, 1].bar(category_counts.index, category_counts.values, color=colors)
    axes[0, 1].set_xlabel('Kategori')
    axes[0, 1].set_ylabel('Frekans')
    axes[0, 1].set_title('Kategori DaÄŸÄ±lÄ±mÄ±')
    axes[0, 1].tick_params(axis='x', rotation=45)
    axes[0, 1].grid(True, alpha=0.3, axis='y')
    
    # Her Ã§ubuÄŸa sayÄ± ekle
    for i, (cat, count) in enumerate(category_counts.items()):
        axes[0, 1].text(i, count + 50, str(count), ha='center', va='bottom', fontweight='bold')
    
    # 5.3 Metin uzunluklarÄ±
    df['text_length'] = df['text'].apply(lambda x: len(str(x).split()))
    axes[0, 2].hist(df['text_length'], bins=20, edgecolor='black', alpha=0.7, color='orange')
    axes[0, 2].axvline(x=df['text_length'].mean(), color='red', linestyle='--', 
                      label=f'Ortalama: {df["text_length"].mean():.1f}')
    axes[0, 2].set_xlabel('Metin UzunluÄŸu (kelime)')
    axes[0, 2].set_ylabel('Frekans')
    axes[0, 2].set_title('Metin UzunluklarÄ± DaÄŸÄ±lÄ±mÄ±')
    axes[0, 2].legend()
    axes[0, 2].grid(True, alpha=0.3)
    
    # 5.4 Skor vs Uzunluk scatter
    axes[1, 0].scatter(df['sentiment_score'], df['text_length'], alpha=0.3, s=20, color='purple')
    axes[1, 0].set_xlabel('Duygu Skoru')
    axes[1, 0].set_ylabel('Metin UzunluÄŸu')
    axes[1, 0].set_title('Skor vs Metin UzunluÄŸu')
    axes[1, 0].grid(True, alpha=0.3)
    
    # 5.5 Zaman iÃ§inde skor deÄŸiÅŸimi
    df['date'] = pd.to_datetime(df['date'])
    df_sorted = df.sort_values('date')
    
    # AylÄ±k ortalama skor
    df_sorted['year_month'] = df_sorted['date'].dt.to_period('M')
    monthly_avg = df_sorted.groupby('year_month')['sentiment_score'].mean().reset_index()
    monthly_avg['year_month'] = monthly_avg['year_month'].astype(str)
    
    axes[1, 1].plot(range(len(monthly_avg)), monthly_avg['sentiment_score'], 
                   marker='o', linewidth=2, color='blue')
    axes[1, 1].set_xlabel('Ay')
    axes[1, 1].set_ylabel('Ortalama Duygu Skoru')
    axes[1, 1].set_title('Zaman Ä°Ã§inde Duygu DeÄŸiÅŸimi')
    axes[1, 1].axhline(y=0, color='red', linestyle='--', alpha=0.5)
    axes[1, 1].grid(True, alpha=0.3)
    
    # X eksenini ayarlama (her 3 ayda bir gÃ¶ster)
    tick_positions = range(0, len(monthly_avg), 3)
    tick_labels = [monthly_avg.iloc[i]['year_month'] for i in tick_positions]
    axes[1, 1].set_xticks(tick_positions)
    axes[1, 1].set_xticklabels(tick_labels, rotation=45)
    
    # 5.6 Åirket bazlÄ± skorlar
    company_scores = df.groupby('company')['sentiment_score'].mean().sort_values()
    top_10_companies = pd.concat([company_scores.head(5), company_scores.tail(5)])
    
    colors_bar = ['#ff6666' if score < 0 else '#00cc00' for score in top_10_companies.values]
    axes[1, 2].barh(range(len(top_10_companies)), top_10_companies.values, color=colors_bar)
    axes[1, 2].set_yticks(range(len(top_10_companies)))
    axes[1, 2].set_yticklabels(top_10_companies.index)
    axes[1, 2].set_xlabel('Ortalama Duygu Skoru')
    axes[1, 2].set_title('Åirket BazlÄ± Ortalama Duygu SkorlarÄ±\n(En Olumsuz ve En Olumlu 5 Åirket)')
    axes[1, 2].axvline(x=0, color='black', linestyle='--', alpha=0.5)
    axes[1, 2].grid(True, alpha=0.3, axis='x')
    
    plt.tight_layout()
    
    # GÃ¶rselleri kaydet
    output_dir = 'data/processed'
    os.makedirs(output_dir, exist_ok=True)
    
    plt.savefig(f'{output_dir}/data_analysis.png', dpi=150, bbox_inches='tight')
    plt.savefig(f'{output_dir}/data_analysis.pdf', bbox_inches='tight')
    plt.show()
    
    print(f"   âœ… GÃ¶rselleÅŸtirmeler kaydedildi: {output_dir}/data_analysis.png")
    
    # 6. Veriyi hazÄ±rla ve tokenize et
    print("\n6. Veri hazÄ±rlanÄ±yor (temizleme, tokenization, split)...")
    processed_data = prepare_data(df, config)
    
    # 7. Ä°ÅŸlenmiÅŸ verileri kaydet
    print("\n7. Ä°ÅŸlenmiÅŸ veriler kaydediliyor...")
    
    # processed_data.pickle olarak kaydet
    processed_path = f'{output_dir}/processed_data.pickle'
    with open(processed_path, 'wb') as f:
        pickle.dump(processed_data, f)
    
    print(f"   âœ… Ä°ÅŸlenmiÅŸ veriler kaydedildi: {processed_path}")
    
    # 8. Ä°statistikleri yazdÄ±r
    print("\n8. Ä°ÅLEME Ä°STATÄ°STÄ°KLERÄ°:")
    print("=" * 60)
    print(f"   Train seti:      {len(processed_data['X_train']):>6} Ã¶rnek")
    print(f"   Validation seti: {len(processed_data['X_val']):>6} Ã¶rnek")
    print(f"   Test seti:       {len(processed_data['X_test']):>6} Ã¶rnek")
    print(f"   Toplam:          {len(processed_data['X_train']) + len(processed_data['X_val']) + len(processed_data['X_test']):>6} Ã¶rnek")
    print()
    print(f"   Kelime daÄŸarcÄ±ÄŸÄ± boyutu: {processed_data['vocab_size']}")
    print(f"   Sequence uzunluÄŸu:       {processed_data['X_train'].shape[1]}")
    print(f"   Embedding boyutu:        {config['model']['embedding_dim']}")
    
    # 9. Train setinden Ã¶rnek gÃ¶ster
    print("\n9. Ã–RNEK TOKENIZATION:")
    print("-" * 40)
    
    sample_idx = 42  # Rastgele bir Ã¶rnek
    tokenizer = processed_data['tokenizer']
    
    # Orijinal metni bul
    original_text = df.iloc[sample_idx]['text']
    cleaned_text = df.iloc[sample_idx]['cleaned_text']
    true_score = processed_data['y_train'][sample_idx] if sample_idx < len(processed_data['y_train']) else "N/A"
    
    # Sequence'i al
    if sample_idx < len(processed_data['X_train']):
        sample_sequence = processed_data['X_train'][sample_idx]
        
        # SÄ±fÄ±r olmayan token'larÄ± al
        non_zero_tokens = sample_sequence[sample_sequence != 0]
        
        print(f"   Ã–rnek Index: {sample_idx}")
        print(f"   Orijinal metin: {original_text}")
        print(f"   TemizlenmiÅŸ: {cleaned_text}")
        print(f"   GerÃ§ek skor: {true_score}")
        print(f"   Token sayÄ±sÄ±: {len(non_zero_tokens)}")
        print(f"   Token IDs (ilk 10): {non_zero_tokens[:10].tolist()}")
        
        # ID'leri kelimelere Ã§evir
        if hasattr(tokenizer, 'index_word'):
            words = [tokenizer.index_word.get(token_id, f'[UNK_{token_id}]') 
                    for token_id in non_zero_tokens[:10]]
            print(f"   Kelimeler (ilk 10): {words}")
    
    print("\n" + "=" * 60)
    print("ğŸ‰ VERÄ° Ã–N Ä°ÅLEME BAÅARIYLA TAMAMLANDI!")
    print("=" * 60)
    print("\nğŸ“ OLUÅTURULAN DOSYALAR:")
    print(f"   1. data/processed/processed_data.pickle")
    print(f"   2. models/tokenizer.pickle")
    print(f"   3. data/processed/data_analysis.png")
    print(f"   4. data/processed/data_analysis.pdf")
    
    # 10. Model iÃ§in hazÄ±rlÄ±k bilgileri
    print("\nğŸ”§ MODEL EÄÄ°TÄ°MÄ° Ä°Ã‡Ä°N HAZIRLIK:")
    print("-" * 40)
    print(f"   Batch size:        {config['training']['batch_size']}")
    print(f"   Epoch sayÄ±sÄ±:      {config['training']['epochs']}")
    print(f"   Learning rate:     {config['training']['learning_rate']}")
    print(f"   Embedding boyutu:  {config['model']['embedding_dim']}")
    print(f"   FastText kullanÄ±mÄ±: {'EVET' if config['model']['use_fasttext'] else 'HAYIR'}")

if __name__ == "__main__":
    main()